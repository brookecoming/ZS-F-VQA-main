
# ZS-F-VQA

![version](https://img.shields.io/badge/version-1.0.1-blue)
[![license](https://img.shields.io/github/license/mashape/apistatus.svg?maxAge=2592000)](https://github.com/China-UK-ZSL/ZS-F-VQA/blob/main/LICENSE)
[![arxiv badge](https://img.shields.io/badge/arXiv-2107.05348-red)](http://arxiv.org/abs/2107.05348)

 

## åŸºç¡€çŸ¥è¯†VQAå¤„ç†æµç¨‹

![](./assets/vqa%20procedure.png)

## ğŸ””  æ–‡ç« æ¥æº: Zero-shot Visual Question Answering using Knowledge Graph

[ISWC 2021 ](https://arxiv.org/abs/2107.05348), or url: https://arxiv.org/abs/2107.05348

> [!IMPORTANT]
>
> Abstract: 
> In this work, we propose a Zero-shot VQA algorithm using knowledge graphs and a mask-based learning mechanism for better incorporating external knowledge, and present new answer-based Zero-shot VQA splits for the F-VQA dataset.

## ğŸ”” æ–°é—»

- `2024-02`We preprint our Survey at arxiv: [Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey](http://arxiv.org/abs/2402.05391); or https://arxiv.org/abs/2402.05391
  
- [`Repo for the servey `](https://github.com/zjukg/KG-MM-Survey), or https://github.com/zjukg/KG-MM-Survey

## ğŸ”” æ¨¡å‹æ¶æ„

![Model_architecture](./assets/Model_architecture.png)

## ğŸ”” ä½¿ç”¨è¯´æ˜

### 1   ç³»ç»Ÿéœ€è¦

```
python >= 3.5
PyTorch >= 1.6.0
```

For more detail of requirements:  
`cmd/bash> pip install -r requirements.txt`

my version (python3.10.5):

```
h5py==3.10.0
easydict==1.13
numpy==1.26.4
tqdm==4.50.2
Unidecode==1.3.8<br>
nltk==3.5<br>
matplotlib==3.8.3<br>
wordninja==2.0.0<br>
torch==2.2.2<br>
torchvision==0.17.2<br>
pandas==2.2.1<br>
six==1.16.0<br>
Pillow==10.3.0<br>
python_Levenshtein==0.25.0<br>
SciencePlots==2.1.1<br>
Levenshtein==0.25.0<br>
tensorboard==2.16.2<br>
pytest==8.1.1<br>
pytest-mock==3.14.0
```

### 2. æ•°æ®å‡†å¤‡

1.  **Location of 5 F-VQA train / test data split:<br>**

- ```data/KG_VQA/fvqa/exp_data/train_data```  <br>
  //è®­ç»ƒé›†å·²å‘æ”¾, å¦‚: all_qs_dict_release_train.json, all_qs_dict_release_train_500.json
- ```data/KG_VQA/fvqa/exp_data/test_data```  <br>
  //æµ‹è¯•é›†å·²å‘æ”¾, å¦‚: all_qs_dict_release_test.json, all_qs_dict_release_test_500.json

2. **Location of 5 ZS-F-VQA train / test data split:<br>**

- ```data/KG_VQA/fvqa/exp_data/train_seen_data``` <br>
  //è®­ç»ƒé›†å·²å‘æ”¾, å¦‚: all_qs_dict_release_train_500.json
- ```data/KG_VQA/fvqa/exp_data/test_unseen_data``` <br>
  //æµ‹è¯•é›†å·²å‘æ”¾, å¦‚: all_qs_dict_release_test_500.json


3. **Answers are available at ``data/KG_VQA/data/FVQA/new_dataset_release/.`` <br>**
   //ç­”æ¡ˆå·²å‘æ”¾, å¦‚: all_fact_triples_release.json, all_qs_dict_release.json, all_qs.dict_release_combine.json,
   ans_tntity_map.txtç­‰ç­‰

> [!NOTE]
 **è®­ç»ƒé›†å’Œæµ‹è¯•é›†è¯´æ˜**
  **è®­ç»ƒé›†ï¼ˆtrain_data å’Œ train_seen_dataï¼‰ï¼š**
  ç”¨äºè®­ç»ƒæ¨¡å‹çš„æ•°æ®é›†ã€‚è¿™äº›æ•°æ®åŒ…æ‹¬å›¾ç‰‡ã€ç›¸å…³çš„é—®é¢˜å’Œæ­£ç¡®çš„ç­”æ¡ˆã€‚æ¨¡å‹é€šè¿‡å­¦ä¹ è¿™äº›é—®é¢˜ä¸ç­”æ¡ˆçš„å¯¹åº”å…³ç³»ï¼Œå°è¯•ç†è§£å’Œå­¦ä¹ å¦‚ä½•ä»ç»™å®šçš„å›¾åƒä¸­æå–ä¿¡æ¯ä»¥å›ç­”é—®é¢˜ã€‚
  train_data ç”¨äºå¸¸è§„çš„è®­ç»ƒã€‚
  train_seen_data åœ¨é›¶æ ·æœ¬å­¦ä¹ ï¼ˆZSLï¼‰çš„ä¸Šä¸‹æ–‡ä¸­ä½¿ç”¨ï¼Œæ„å‘³ç€è¿™éƒ¨åˆ†æ•°æ®åŒ…å«äº†åœ¨è®­ç»ƒé˜¶æ®µâ€œçœ‹åˆ°â€çš„æ ·æœ¬ã€‚ 

   **æµ‹è¯•é›†ï¼ˆtest_data å’Œ test_unseen_dataï¼‰ï¼š**
   ç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½çš„æ•°æ®é›†ã€‚è¿™éƒ¨åˆ†æ•°æ®æ¨¡å‹åœ¨è®­ç»ƒé˜¶æ®µæœªæ›¾â€œçœ‹åˆ°â€ï¼Œç›®çš„æ˜¯æµ‹è¯•æ¨¡å‹å¯¹æ–°é—®é¢˜å’Œå›¾åƒçš„å›åº”èƒ½åŠ›ã€‚
   test_data æ˜¯å¸¸è§„æµ‹è¯•é›†ï¼Œç”¨æ¥è¯„ä¼°æ¨¡å‹åœ¨çœ‹è¿‡çš„é—®é¢˜ç±»å‹ä¸Šçš„æ€§èƒ½ã€‚
   test_unseen_data åœ¨é›¶æ ·æœ¬å­¦ä¹ çš„åœºæ™¯ä¸‹ä½¿ç”¨ï¼ŒåŒ…å«æ¨¡å‹åœ¨è®­ç»ƒæ—¶æœªè§è¿‡çš„é—®é¢˜æˆ–æ¦‚å¿µï¼Œç”¨äºè¯„ä¼°æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚"

### 3 å›¾ç‰‡å‡†å¤‡

#### å¯¹Image folderæ“ä½œ

> - `data/KG_VQA/fvqa/exp_data/images/images``` æ­¤å¤„æ˜¯ä¸ªç©ºæ–‡ä»¶å¤¹ï¼Œ
> - put all your `.JPEG`/`.jpg` file here: C:\ZS-F-VQA-main\data\KG_VQA\fvqa\exp_data\images\images <br>
> - éœ€æ‰‹åŠ¨æ·»åŠ h5æ–‡ä»¶è¿›ä¸Šè¿°image folder:C:
>   \ZS-F-VQA-main\data\KG_VQA\fvqa\exp_data\images\images\fvqa-resnet-14x14.h5 <br>

#### H5æ–‡ä»¶è§£é‡Šï¼š

'''
h5æ–‡ä»¶æ˜¯resneté¢„å¤„ç†é¢„è®­ç»ƒåå¾—åˆ°çš„ç‰¹å¾ä¿¡æ¯.HDF5æ–‡ä»¶é¡¶å±‚åŒ…å«ä¸¤ä¸ªæ•°æ®é›†ï¼šâ€œfeaturesâ€å’Œâ€œidsâ€ã€‚åœ¨è®¸å¤šæœºå™¨å­¦ä¹ æ•°æ®é›†ä¸­ï¼Œè¿™æ˜¯å¸¸è§çš„ç»“æ„.å…¶ä¸­featuresè¿™ä¸ªæ•°æ®é›†å¯èƒ½åŒ…å«å®é™…çš„å›¾åƒæ•°æ®æˆ–ä»å›¾åƒä¸­æå–çš„ç‰¹å¾ã€‚è¿™äº›æ•°æ®å¯èƒ½æ˜¯åŸå§‹çš„åƒç´ å€¼ï¼Œä¹Ÿå¯èƒ½æ˜¯ç»è¿‡æŸç§å¤„ç†ï¼ˆå¦‚ç‰¹å¾æå–ç®—æ³•å¤„ç†ï¼‰åçš„æ•°å€¼æ•°æ®ã€‚ä¾‹å¦‚ï¼Œè¿”å›å€¼(
10000, 32, 32, 3)è¡¨ç¤ºè¿™æ˜¯ä¸€ä¸ªå››ç»´æ•°ç»„ï¼ŒåŒ…å«10000ä¸ª32x32åƒç´ å¤§å°ä¸”è‰²å½©é€šé“ä¸º3ï¼ˆRGBï¼‰çš„å›¾åƒã€‚
ids: è¿™ä¸ªæ•°æ®é›†å¯èƒ½åŒ…å«ä¸â€œfeaturesâ€æ•°æ®é›†ä¸­æ¯ä¸ªå›¾åƒæˆ–ç‰¹å¾å‘é‡ç›¸å¯¹åº”çš„æ ‡è¯†ç¬¦ã€‚<br>

- è¿™é‡Œï¼Œfeatures æ•°æ®é›†ç»´åº¦æ˜¯ (3016, 2048, 14, 14),è¿™ä¸ªæ•°æ®é›†å¯èƒ½æ˜¯é€šè¿‡ä¸€ä¸ªCNNï¼ˆå¦‚ResNetï¼‰æ¨¡å‹å¾—åˆ°çš„ï¼Œå…¶ä¸­åŒ…å«äº† 3016
  å¼ å›¾åƒç»è¿‡æ¨¡å‹å¤„ç†åçš„ç‰¹å¾å›¾ï¼ˆæ¯å¼ å›¾åƒå¯¹åº” 2048 ä¸ªç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾æ˜¯ä¸€ä¸ªå¤§å°ä¸º 14x14 çš„å°å›¾åƒï¼‰:<br>
    - ç¬¬ä¸€ç»´ï¼šåŒ…å« 3016 ä¸ªå…ƒç´ ï¼Œå¯èƒ½ä»£è¡¨æœ‰ 3016 ä¸ªæ ·æœ¬ã€‚<br>
    - ç¬¬äºŒç»´ï¼šåŒ…å« 2048 ä¸ªå…ƒç´ ï¼Œå¯èƒ½ä»£è¡¨æ¯ä¸ªæ ·æœ¬æœ‰ 2048 ä¸ªç‰¹å¾ã€‚<br>
    - ç¬¬ä¸‰ç»´ï¼šåŒ…å« 14 ä¸ªå…ƒç´ ï¼Œé…åˆç¬¬å››ç»´ï¼Œå¯èƒ½ä»£è¡¨æ¯ä¸ªç‰¹å¾æ˜¯ä¸€ä¸ª 14x14 çš„å°å›¾åƒã€‚'''

- Image feature:
  - `fvqa-resnet-14x14.h5`è¿™ä¸ªæ–‡ä»¶ä»ä¸‹é¢ç½‘å€å¾—åˆ°ï¼Œè¿˜éœ€è¦æ·»åŠ åˆ°ä¸Šé¢çš„æ–‡ä»¶å¤¹å».
    pretrained: [GoogleDrive](https://drive.google.com/file/d/1YG9hByw01_ZQ6_mKwehYiddG3x2Cxatu/view?usp=sharing)
    or [BaiduCloud(password:16vd)](https://pan.baidu.com/s/1ks84AWSXxJJ_7LwnzWdEnQ) https://pan.baidu.com/s/1ks84AWSXxJJ_7LwnzWdEnQ
  - `fvqa36_imgid2idx.pkl` and `fvqa_36.hdf5`
    pretrained: [GoogleDrive](https://drive.google.com/file/d/1wfgmPhNF7DR7_yEAr8lxjtdsko7lLCWj/view?usp=sharing)
    or [BaiduCloud](https://pan.baidu.com/s/11KRiw2jvPBzgd3xUbynHjw?pwd=zsqa) (password:zsqa)

  - Origin images are available at [FVQA](https://github.com/wangpengnorman/FVQA)
    with [download_link](https://www.dropbox.com/s/iyz6l7jhbt6jb7q/new_dataset_release.zip?dl=0).
  - Other vqa dataset: you could generate a pretrained image feature via this
    way ([Guidance](https://github.com/hexiang-hu/answer_embedding/issues/3) / [code](https://github.com/Cyanogenoid/pytorch-vqa/blob/master/preprocess-images.py))
  - The generated `.h` file should be placed in :
    ```data/KG_VQA/fvqa/exp_data/common_data/.```

**Answer / Qusetion vocab:**

- The generated file `answer.vocab.fvqa.json` & `question.vocab.fvqa.json`  now are available at :
  ```data/KG_VQA/fvqa/exp_data/common_data/.```
- Other vqa dataset: code
  for [process answer vocab](https://github.com/hexiang-hu/answer_embedding/blob/master/tools/preprocess_answer.py)
  and [process questions vocab](https://github.com/hexiang-hu/answer_embedding/blob/master/tools/preprocess_question.py)

---

### Pretrained Model ([url](https://www.dropbox.com/sh/vp5asuivqpiir5w/AAC3k_gELrP4ydNNok_o1vlYa?dl=0))

Download it and overwrite ```data/KG_VQA/fvqa/model_save```

### [Parameter](#content)

```
[--KGE {TransE,ComplEx,TransR,DistMult}] [--KGE_init KGE_INIT] [--GAE_init GAE_INIT] [--ZSL ZSL] [--entity_num {all,4302}] [--data_choice {0,1,2,3,4}]
               [--name NAME] [--no-tensorboard] --exp_name EXP_NAME [--dump_path DUMP_PATH] [--exp_id EXP_ID] [--random_seed RANDOM_SEED] [--freeze_w2v {0,1}]
               [--ans_net_lay {0,1,2}] [--fact_map {0,1}] [--relation_map {0,1}] [--now_test {0,1}] [--save_model {0,1}] [--joint_test_way {0,1}] [--top_rel TOP_REL]
               [--top_fact TOP_FACT] [--soft_score SOFT_SCORE] [--mrr MRR]
```

Available model for training: ```Up-Down```, `BAN`, `SAN`, `MLP`

**You can try your own model via adding it (`.py`) to :** `main/code/model/.`

For more details: ```code/config.py```

---

### Running

```cd code```

**For data check:**

- ```python deal_data.py --exp_name data_check```

**General VQA:**

- train:
  ```bash run_FVQA_train.sh```
- test:
  ```bash run_FVQA.sh```

**ZSL/GZSL VQA:**

- train:
  ```bash run_ZSL_train.sh```
- test:
  ```bash run_ZSL.sh```

**Note**:

- you can open the `.sh` file for <a href="#Parameter">parameter</a> modification.

**Result:**

- Log file will be saved to: ```code/dump```
- model will be saved to: ```data/KG_VQA/fvqa/model_save```

<br />

## Explainable

![explainable](https://github.com/China-UK-ZSL/ZS-F-VQA/blob/main/figures/all_explainable.png)

<br />

## Acknowledgements

Thanks for the following released works:

> [SciencePlots](https://github.com/garrettj403/SciencePlots), [ramen](https://github.com/erobic/ramen), [GAE](https://github.com/zfjsail/gae-pytorch), [vqa-winner-cvprw-2017](https://github.com/markdtw/vqa-winner-cvprw-2017), [faster-rcnn](https://github.com/jwyang/faster-rcnn.pytorch), [VQA](https://github.com/Shivanshu-Gupta/Visual-Question-Answering), [BAN](https://github.com/jnhwkim/ban-vqa), [commonsense-kg-completion](https://github.com/allenai/commonsense-kg-completion), [bottom-up-attention-vqa](https://github.com/hengyuan-hu/bottom-up-attention-vqa), [FVQA](https://github.com/wangpengnorman/FVQA), [answer_embedding](https://github.com/hexiang-hu/answer_embedding), [torchlight](https://github.com/RamonYeung/torchlight)

## Cite:

Please condiser citing this paper if you use the code

```bigquery
@inproceedings{chen2021zero,
  title={Zero-Shot Visual Question Answering Using Knowledge Graph},
  author={Chen, Zhuo and Chen, Jiaoyan and Geng, Yuxia and Pan, Jeff Z and Yuan, Zonggang and Chen, Huajun},
  booktitle={International Semantic Web Conference},
  pages={146--162},
  year={2021},
  organization={Springer}
}
```

For more details, please submit a issue or contact [Zhuo Chen](https://github.com/hackerchenzhuo).
<a href="https://info.flagcounter.com/VOlE"><img src="https://s11.flagcounter.com/count2/VOlE/bg_FFFFFF/txt_000000/border_F7F7F7/columns_6/maxflags_12/viewers_3/labels_0/pageviews_0/flags_0/percent_0/" alt="Flag Counter" border="0"></a>
